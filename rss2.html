<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>JoelYing</title>
    <link>https://joelying.github.io/</link>
    <atom:link href="/rss2.html" rel="self" type="application/rss+xml"/>
    
    <description>半个兴趣使然的程序员</description>
    <pubDate>Sat, 25 Jan 2020 06:28:34 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>部署Scrapy项目到腾讯云服务器</title>
      <link>https://joelying.github.io//blog/%E9%83%A8%E7%BD%B2Scrapy%E9%A1%B9%E7%9B%AE%E5%88%B0%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.html</link>
      <guid>https://joelying.github.io//blog/%E9%83%A8%E7%BD%B2Scrapy%E9%A1%B9%E7%9B%AE%E5%88%B0%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.html</guid>
      <pubDate>Fri, 24 Jan 2020 10:55:03 GMT</pubDate>
      <description>
      
        当我轻轻的放下你，把一切重新整理去归零，可以坦然走进，只有你我的电梯
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- build time:Sat Jan 25 2020 14:31:20 GMT+0800 (GMT+08:00) --><img class="joel-img" src="http://image.joelyings.com/2020-01-25_10.jpg"><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>打算把写完的爬虫项目放到服务器上定时运行，然后了解到有<code>scrapyd</code>这个方便管理爬虫，于是这篇文章的指向是在腾讯云服务器上运行<code>scrapd</code>，然后把我们的爬虫上传到<code>scrapyd</code>，使得<code>scrapyd</code>可以管理爬虫项目(注：没有通过文件传输工具把scrapy爬虫项目的文件上传到服务器，额，我是这么理解的，但是是通过<code>scrapyd</code>上传的egg)</p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>服务器是刚买的腾讯云的<code>CentOS 7</code>系统，没有云数据库，所以后面自己搭<code>mysql</code>，有<code>python2</code>但是没有<code>python3</code></p><h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h4><p>在本机下载scrapyd-client：</p><p>对于windows系统，建议不要用<code>pip install scrapyd-client</code>去安装<code>scrapyd-client</code>，会出现，scrapyd-deploy不是内部或外部命令，因为scrapyd-deploy不能被windows执行</p><p>应当直接去<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">github</a>上下载并解压安装包后，进入解压后的目录下，执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>进行安装，若已经用pip安装了的，先卸载Scrapyd-client</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip list</span><br><span class="line">pip uninstall scrapyd-client</span><br></pre></td></tr></table></figure><h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4><p>登陆腾讯云服务器里面进行相应的python和相关库安装(结合几篇博文的命令如下)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入home目录</span></span><br><span class="line">cd ~</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装相关库</span></span><br><span class="line">yum -y groupinstall <span class="string">"Development tools"</span></span><br><span class="line">yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel</span><br><span class="line">yum -y install gcc</span><br><span class="line">yum install -y libffi-devel zlib1g-dev</span><br><span class="line">yum install zlib* -y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 下载python3.6安装包</span></span><br><span class="line">wget https://www.python.org/ftp/python/<span class="number">3.6</span><span class="number">.8</span>/Python<span class="number">-3.6</span><span class="number">.8</span>.tgz</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建一个文件夹</span></span><br><span class="line">mkdir /usr/local/python3</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 解压安装包</span></span><br><span class="line">tar -zxvf Python<span class="number">-3.6</span><span class="number">.8</span>.tgz</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 进入解压后的目录</span></span><br><span class="line">cd Python<span class="number">-3.6</span><span class="number">.8</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 配置，使安装路径为/usr/local/python3.6</span></span><br><span class="line"><span class="comment"># 第一个指定安装的路径,不指定的话,安装过程中可能软件所需要的文件复制到其他不同目录,删除软件很不方便,复制软件也不方便.</span></span><br><span class="line"><span class="comment"># 第二个可以提高python10%-20%代码运行速度. 参考：https://blog.csdn.net/whatday/article/details/98053179</span></span><br><span class="line"><span class="comment"># 第三个是为了安装pip需要用到ssl,后面报错会有提到.</span></span><br><span class="line">./configure --prefix=/usr/local/python3 --enable-optimizations --<span class="keyword">with</span>-ssl</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 编译，安装 时间较长 </span></span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建软连接</span></span><br><span class="line">ln -s /usr/local/python3/bin/python3 /usr/local/bin/python3</span><br><span class="line">ln -s /usr/local/python3/bin/pip3 /usr/local/bin/pip3</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 安装scrapyd,scrapyd-client和scrapy</span></span><br><span class="line">pip3 install scrapyd</span><br><span class="line">pip3 install scrapy</span><br><span class="line">pip3 install scrapyd-client</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新pip</span></span><br><span class="line">pip3 install --upgrade pip</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 安装项目需要的库</span></span><br><span class="line">pip install requests</span><br></pre></td></tr></table></figure><p>使得外网能够访问服务器IP，修改如下文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找default_scrapyd.conf路径</span></span><br><span class="line">find / -name default_scrapyd.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改default_scrapyd.conf，使外网IP可以访问</span></span><br><span class="line">vi /usr/local/python3/lib/python3<span class="number">.6</span>/site-packages/scrapyd/default_scrapyd.conf</span><br></pre></td></tr></table></figure><p>default_scrapyd.conf文件里的bind_address修改为</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind_address = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><p>然后进入腾讯云控制台，点击安全组—&gt;再点击新建</p><p><img src="http://image.joelyings.com/2020-01-25_1.png" alt></p><p><img src="http://image.joelyings.com/2020-01-25_2.png" alt></p><p>在新建的安全组点击<code>修改规则</code>然后<code>添加规则</code></p><p><img src="http://image.joelyings.com/2020-01-25_3.png" alt></p><p>保存后返回服务器实例页面，选中实例，点击<code>更多操作</code>，加入安全组，添加刚才创建安全组</p><p><img src="http://image.joelyings.com/2020-01-25_4.png" alt></p><p>此时用 <code>&#39;/usr/local/python3.6/bin/scrapyd&#39;</code>启动scrapyd，加上nohup&amp;则在后台启动运行</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 查看scrapy</span></span><br><span class="line"><span class="string">'/usr/local/python3/bin/scrapy'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 启动scrapyd</span></span><br><span class="line"><span class="string">'/usr/local/python3/bin/scrapyd'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 在后台启动运行scrapyd</span></span><br><span class="line">nohup <span class="string">'/usr/local/python3/bin/scrapyd'</span> &amp;</span><br></pre></td></tr></table></figure><p>启动<code>scrapyd</code>成功</p><p><img src="http://image.joelyings.com/2020-01-25_5.png" alt></p><p>后台运行<code>scrapyd</code></p><p><img src="http://image.joelyings.com/2020-01-25_6.png" alt></p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="symbol">@VM_0_14_centos</span> ~]<span class="meta"># nohup <span class="string">'/usr/local/python3/bin/scrapyd'</span> &amp;</span></span><br><span class="line">[<span class="number">1</span>] <span class="number">25485</span></span><br><span class="line">[root<span class="symbol">@VM_0_14_centos</span> ~]<span class="meta"># nohup: ignoring input and appending output to ‘nohup.out’</span></span><br></pre></td></tr></table></figure><p>通过服务器外网IP:6800可以在浏览器里看到如下页面</p><p><img src="http://image.joelyings.com/2020-01-25_7.png" alt></p><h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><p>服务器端安装<code>Mysql 5.7</code></p><p><em>一、配置yum源</em></p><p>1.下载mysql源安装包</p><p>在MySQL官网中下载YUM源rpm安装包：<a href="http://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="noopener">http://dev.mysql.com/downloads/repo/yum/</a></p><p>本次下载目录为：/home/目录，因此进入：cd /home</p><p>执行下载命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http<span class="variable">s:</span>//dev.mysql.<span class="keyword">com</span>/<span class="built_in">get</span>/mysql80-community-release-el7-<span class="number">1</span>.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.安装mysql源</p><p>下载完成后使用下面命令安装源：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum localinstall mysql80-community-release-el7-<span class="number">1</span><span class="selector-class">.noarch</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p>3.检查是否安装成功</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum repolist enabled <span class="string">| grep "</span>mysql.*-community.*<span class="string">"</span></span><br></pre></td></tr></table></figure><p>4.修改安装版本</p><p>默认安装的mysql版本是8.0，需要安装mysql5.7，需要修改<code>/etc/yum.repos.d/mysql-community.repo</code>源，改变默认安装的mysql版本</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span>/mysql-community.repo</span><br></pre></td></tr></table></figure><p>将5.7源的enabled=0改成enabled=1，将8.0的enabled=1改成enabled=0即可</p><p><em>二、安装mysql</em></p><p>直接使用命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> mysql-community-<span class="keyword">server</span></span><br></pre></td></tr></table></figure><p><em>三、启动mysql服务</em></p><p>1.启动</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="literal">start</span> mysqld</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysqld <span class="literal">start</span></span><br></pre></td></tr></table></figure><p>2.查看启动状态</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">systemctl status mysqld</span></span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">service mysqld status</span></span><br></pre></td></tr></table></figure><p>3.设置开机启动</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="builtin-name">enable</span> mysqld</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure><p><em>四、配置及部分命令</em></p><p>1.修改登录密码</p><p>mysql安装完成之后，在<code>/var/log/mysqld.log</code>文件中给root生成了一个默认密码，通过下面的方式找到root默认密码，然后登录mysql进行修改：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep <span class="string">'temporary password'</span> /<span class="built_in">var</span>/<span class="keyword">log</span>/mysqld.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><p>密码就是<code>root@localhost</code>冒号后面的全部字符</p><p>2.本地MySQL客户端登录</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mysql -uroot -p</span></span><br></pre></td></tr></table></figure><p>密码是上一步查询出来的，输入后回车</p><p>然后修改密码：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">password</span> <span class="keyword">for</span> <span class="string">'root'</span>@<span class="string">'localhost'</span>=<span class="keyword">password</span>(<span class="string">'xxxxxxxx'</span>);</span><br></pre></td></tr></table></figure><p>注意：mysql5.7默认安装了密码安全检查插件（validate_password），默认密码检查策略要求密码必须包含：大小写字母、数字和特殊符号，并且长度不能少于8位。否则会提示<code>ERROR 1819 (HY000): Your password does not satisfy the current policy requirements</code>错误</p><p>通过msyql环境变量可以查看密码策略的相关信息（执行这一步需要先修改默认密码，即执行完上一步修改才可以，否则会报错：<code>ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.）</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">variables</span> <span class="keyword">like</span> <span class="string">'%password%'</span>;</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">validate_password_policy：密码策略，默认为MEDIUM策略</span><br><span class="line"></span><br><span class="line">validate_password_dictionary_file：密码策略文件，策略为STRONG才需要</span><br><span class="line"></span><br><span class="line">validate_password_length：密码最少长度</span><br><span class="line"></span><br><span class="line">validate_password_mixed_case_count：大小写字符长度，至少<span class="number">1</span>个</span><br><span class="line"></span><br><span class="line">validate_password_number_count ：数字至少<span class="number">1</span>个</span><br><span class="line"></span><br><span class="line">validate_password_special_char_count：特殊字符至少<span class="number">1</span>个</span><br></pre></td></tr></table></figure><p>上述参数是默认策略MEDIUM的密码检查规则</p><p>修改密码策略：</p><p>在/etc/my.cnf文件添加validate_password_policy配置，指定密码策略：</p><p>选择0（LOW），1（MEDIUM），2（STRONG）其中一种，选择2需要提供密码字典文件</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">validate_password_policy</span>=<span class="number">0</span></span><br></pre></td></tr></table></figure><p>如果不需要密码策略，添加my.cnf文件中添加如下配置禁用即可：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">validate_password</span> = <span class="literal">off</span></span><br></pre></td></tr></table></figure><p>重新启动mysql服务使配置生效：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">systemctl restart mysqld</span></span><br></pre></td></tr></table></figure><p>3.添加远程登录用户</p><p>默认只允许root帐户在本地登录，如果要在其它机器上连接mysql，必须修改root允许远程连接，或者添加一个允许远程连接的帐户</p><p>修改root用户远程访问权限：</p><p>选择 mysql 数据库：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="title">mysql</span>;</span><br></pre></td></tr></table></figure><p>在 mysql 数据库的 user 表中查看当前 root 用户的相关信息：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select host,<span class="built_in"> user </span><span class="keyword">from</span> user;</span><br></pre></td></tr></table></figure><p>查看表格中 root 用户的 host，默认应该显示的 localhost，只支持本地访问，不允许远程访问</p><p>授权 root 用户的所有权限并设置远程访问</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host=<span class="string">'%'</span> <span class="keyword">where</span> <span class="keyword">user</span> =<span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><p>然后使用下面命令使修改生效：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><p>4.修改默认编码方式</p><p>mysql8.0默认编码方式为utf8mb4，因此使用时不需要修改，可使用如下命令查看：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">VARIABLES</span> <span class="keyword">WHERE</span> Variable_name <span class="keyword">LIKE</span> <span class="string">'character_set_%'</span> <span class="keyword">OR</span> Variable_name <span class="keyword">LIKE</span> <span class="string">'collation%'</span>;</span><br></pre></td></tr></table></figure><p>如果需要修改其他编码方式，方法有很多，以下仅为举例</p><p>比如需要修改为utf8mb4，可以使用如下方式：</p><p>修改mysql配置文件my.cnf（windows为my.ini）</p><p>my.cnf一般在etc/mysql/my.cnf位置。找到后请在以下三部分里添加如下内容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[client]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">default-character-set</span> = utf8mb4</span><br><span class="line"></span><br><span class="line"><span class="section">[mysql]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">default-character-set</span> = utf8mb4</span><br><span class="line"></span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">character-set-client-handshake</span> = <span class="literal">FALSE</span></span><br><span class="line"><span class="attr">character-set-server</span> = utf8mb4</span><br><span class="line"><span class="attr">collation-server</span> = utf8mb4_unicode_ci</span><br><span class="line"><span class="attr">init_connect</span>=<span class="string">'SET NAMES utf8mb4'</span></span><br></pre></td></tr></table></figure><p>重启mysql即可</p><p>collation_connection 、collation_database 、collation_server是什么没关系，但必须保证以下这几个变量必须是utf8mb4：</p><p>character_set_client (客户端来源数据使用的字符集)<br>character_set_connection (连接层字符集)<br>character_set_database (当前选中数据库的默认字符集)<br>character_set_results (查询结果字符集)<br>character_set_server (默认的内部操作字符集)</p><p>数据库连接参数中:</p><p>characterEncoding=utf8会被自动识别为utf8mb4，也可以不加这个参数，会自动检测</p><p>而autoReconnect=true是必须加上的</p><h4 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a>第四步</h4><p>部署爬虫到服务器</p><p>1.进入本地项目根目录，用<code>scrapyd-deploy -l</code>生成scrapy.cfg(最新的scrapy项目创建后就自带，无需再生成)，并且修改该文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = LVideoSpider.settings</span><br><span class="line"></span><br><span class="line">[deploy:busishu] <span class="comment"># busishu 是自己给定义的host名称</span></span><br><span class="line">url = http://公网IP:<span class="number">6800</span>/</span><br><span class="line">project = LVideoSpider     <span class="comment"># project名称就是上面.settings的前一部分</span></span><br></pre></td></tr></table></figure><p>2.部署爬虫到服务器</p><p>本地进入scrapy.cfg文件所在目录，执行scrapyd-deploy &lt;host名称&gt; -p &lt;project名称&gt;，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy busishu -p LVideoSpider</span><br></pre></td></tr></table></figure><p>成功后，服务器会返回一个json</p><p>但是，我没成功，因为我的scrapy爬虫配置的MySQL连接信息，连接不上服务器的数据库，配置好后服务器端的数据库中却没有对应的表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Server response (200):</span><br><span class="line">&#123;&quot;node_name&quot;: &quot;VM_0_14_centos&quot;, &quot;status&quot;: &quot;error&quot;, &quot;message&quot;: &quot;Traceback (most recent call last):\n  File \&quot;/usr/local/python3/lib/python3.6/runpy.py\&quot;, line 1</span><br><span class="line">93, in _run_module_as_main\n    \&quot;__main__\&quot;, mod_spec)\n  File \&quot;/usr/local/python3/lib/python3.6/runpy.py\&quot;, line 85, in _run_code\n    exec(code, run_global</span><br><span class="line">s)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapyd/runner.py\&quot;, line 40, in &lt;module&gt;\n    main()\n  File \&quot;/usr/local/python3/lib/python3.6/si</span><br><span class="line">te-packages/scrapyd/runner.py\&quot;, line 37, in main\n    execute()\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/cmdline.py\&quot;, line 145, in exe</span><br><span class="line">cute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/crawler.py\&quot;, line 267, in __init__\n</span><br><span class="line">   super(CrawlerProcess, self).__init__(settings)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/crawler.py\&quot;, line 145, in __init__\n    self</span><br><span class="line">.spider_loader = _get_spider_loader(settings)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/crawler.py\&quot;, line 347, in _get_spider_loader\n</span><br><span class="line">  return loader_cls.from_settings(settings.frozencopy())\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\&quot;, line 61, in from_se</span><br><span class="line">ttings\n    return cls(settings)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\&quot;, line 25, in __init__\n    self._load_all_sp</span><br><span class="line">iders()\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\&quot;, line 47, in _load_all_spiders\n    for module in walk_modules(name):</span><br><span class="line">\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/scrapy/utils/misc.py\&quot;, line 73, in walk_modules\n    submod = import_module(fullpath)\n  File \&quot;/usr</span><br><span class="line">/local/python3/lib/python3.6/importlib/__init__.py\&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \&quot;&lt;fr</span><br><span class="line">ozen importlib._bootstrap&gt;\&quot;, line 994, in _gcd_import\n  File \&quot;&lt;frozen importlib._bootstrap&gt;\&quot;, line 971, in _find_and_load\n  File \&quot;&lt;frozen importlib._boot</span><br><span class="line">strap&gt;\&quot;, line 955, in _find_and_load_unlocked\n  File \&quot;&lt;frozen importlib._bootstrap&gt;\&quot;, line 656, in _load_unlocked\n  File \&quot;&lt;frozen importlib._bootstrap&gt;\&quot;</span><br><span class="line">, line 626, in _load_backward_compatible\n  File \&quot;/tmp/LVideoSpider-1579927825-mwhuhl90.egg/LVideoSpider/spiders/lvideo.py\&quot;, line 15, in &lt;module&gt;\n  File \&quot;/</span><br><span class="line">tmp/LVideoSpider-1579927825-mwhuhl90.egg/LVideoSpider/spiders/lvideo.py\&quot;, line 18, in LvideoSpider\n  File \&quot;/tmp/LVideoSpider-1579927825-mwhuhl90.egg/LVideoS</span><br><span class="line">pider/spiders/source_data.py\&quot;, line 13, in get_source_data\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/pymysql/__init__.py\&quot;, line 94, in Connect</span><br><span class="line">\n    return Connection(*args, **kwargs)\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/pymysql/connections.py\&quot;, line 325, in __init__\n    self.con</span><br><span class="line">nect()\n  File \&quot;/usr/local/python3/lib/python3.6/site-packages/pymysql/connections.py\&quot;, line 599, in connect\n    self._request_authentication()\n  File \&quot;/u</span><br><span class="line">sr/local/python3/lib/python3.6/site-packages/pymysql/connections.py\&quot;, line 861, in _request_authentication\n    auth_packet = self._read_packet()\n  File \&quot;/u</span><br><span class="line">sr/local/python3/lib/python3.6/site-packages/pymysql/connections.py\&quot;, line 684, in _read_packet\n    packet.check_error()\n  File \&quot;/usr/local/python3/lib/pyt</span><br><span class="line">hon3.6/site-packages/pymysql/protocol.py\&quot;, line 220, in check_error\n    err.raise_mysql_exception(self._data)\n  File \&quot;/usr/local/python3/lib/python3.6/site</span><br><span class="line">-packages/pymysql/err.py\&quot;, line 109, in raise_mysql_exception\n    raise errorclass(errno, errval)\npymysql.err.InternalError: (1049, \&quot;Unknown database &apos;lvid</span><br><span class="line">eo&apos;\&quot;)\n&quot;&#125;</span><br></pre></td></tr></table></figure><p>所以我尝试在服务器端的数据库创建爬虫所需要的表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># 新建数据库lvideo</span><br><span class="line">create database lvideo default character set utf8mb4 collate utf8mb4_general_ci;</span><br><span class="line"></span><br><span class="line"># 使用该数据库</span><br><span class="line">use lvideo;</span><br><span class="line"></span><br><span class="line"># 创建三表</span><br><span class="line">CREATE TABLE `video_source` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `domin` varchar(128) NOT NULL,</span><br><span class="line">  `name` varchar(128) NOT NULL,</span><br><span class="line">  `type` int(10) unsigned NOT NULL,</span><br><span class="line">  `is_effect` int(10) unsigned NOT NULL,</span><br><span class="line">  `format_page` varchar(512) NOT NULL,</span><br><span class="line">  `created_time` datetime(6) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `video_videoinfo` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(512) NOT NULL,</span><br><span class="line">  `alias` varchar(512) NOT NULL,</span><br><span class="line">  `cover_url` varchar(512) NOT NULL,</span><br><span class="line">  `director` varchar(512) NOT NULL,</span><br><span class="line">  `actor` varchar(1024) NOT NULL,</span><br><span class="line">  `first_type` varchar(256) NOT NULL,</span><br><span class="line">  `second_type` varchar(256) NOT NULL,</span><br><span class="line">  `region` varchar(256) NOT NULL,</span><br><span class="line">  `update_time` varchar(128) NOT NULL,</span><br><span class="line">  `nums` int(10) unsigned NOT NULL,</span><br><span class="line">  `release_time` varchar(64) NOT NULL,</span><br><span class="line">  `intro` longtext NOT NULL,</span><br><span class="line">  `source` varchar(128) NOT NULL,</span><br><span class="line">  `created_time` varchar(128) NOT NULL,</span><br><span class="line">  `remark` varchar(512) NOT NULL,</span><br><span class="line">  `pv` int(10) unsigned NOT NULL,</span><br><span class="line">  `uv` int(10) unsigned NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `video_videoinfo_name_3495f1c0` (`name`),</span><br><span class="line">  KEY `video_videoinfo_alias_0b3eabbd` (`alias`),</span><br><span class="line">  KEY `video_videoinfo_release_time_8393c554` (`release_time`),</span><br><span class="line">  KEY `update_time` (`update_time`) USING BTREE</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=38974 DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `video_videolink` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(512) NOT NULL,</span><br><span class="line">  `link` varchar(512) NOT NULL,</span><br><span class="line">  `number` varchar(128) NOT NULL,</span><br><span class="line">  `is_new` int(10) unsigned NOT NULL,</span><br><span class="line">  `status` int(10) unsigned NOT NULL,</span><br><span class="line">  `source` varchar(128) NOT NULL,</span><br><span class="line">  `created_time` varchar(128) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `name` (`name`) USING BTREE,</span><br><span class="line">  KEY `link` (`link`) USING BTREE,</span><br><span class="line">  KEY `number` (`number`) USING BTREE</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=235037 DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line"># 插入起始所需的数据源</span><br><span class="line">insert into video_source(`domin`,`name`,`type`,`is_effect`,`format_page`,`created_time`) values(&apos;kuyunzy.tv&apos;,&apos;kuyun&apos;,1,1,&apos;http://www.kuyunzy.tv/?m=vod-index-pg-&#123;&#125;.html&apos;,&apos;2020-01-07 09:07:41.466798&apos;);</span><br></pre></td></tr></table></figure><p>然后成功再执行命令就成功了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server response (<span class="number">200</span>):</span><br><span class="line">&#123;<span class="string">"node_name"</span>: <span class="string">"VM_0_14_centos"</span>, <span class="string">"status"</span>: <span class="string">"ok"</span>, <span class="string">"project"</span>: <span class="string">"LVideoSpider"</span>, <span class="string">"version"</span>: <span class="string">"1579928601"</span>, <span class="string">"spiders"</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure><p>本地检查爬虫是否部署成功，这里的<host>实际上是你deploy的目标，不是项目名称</host></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy -L &lt;host&gt;</span><br></pre></td></tr></table></figure><p>若有爬虫成功部署则会返回你部署的项目名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">D:\Python\PycharmProject\LVideoSpider&gt;scrapyd-deploy -L busishu</span><br><span class="line">LVideoSpider</span><br></pre></td></tr></table></figure><p>也可以用<code>scrapyd-deploy -l</code>查看</p><p>检查在服务器上部署的项目</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">http:</span><span class="comment">//&lt;服务器公网IP&gt;:6800/listprojects.json</span></span><br></pre></td></tr></table></figure><p>检查服务器上部署的某个项目的爬虫</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://<span class="tag">&lt;<span class="name">IP</span>&gt;</span>:6800/listspiders.json?project=<span class="tag">&lt;<span class="name">项目名称</span>&gt;</span></span><br></pre></td></tr></table></figure><p>删除服务器上部署的项目</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://<span class="tag">&lt;<span class="name">IP</span>&gt;</span>:6800/delproject.json -d project=<span class="tag">&lt;<span class="name">项目名称</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="第五步"><a href="#第五步" class="headerlink" title="第五步"></a>第五步</h4><p>在服务器端操作爬虫</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行爬虫</span></span><br><span class="line">curl http://<span class="variable">&lt;公网IP&gt;</span>:6800/schedule.json -d project=<span class="variable">&lt;项目名称&gt;</span> -d spider=<span class="variable">&lt;爬虫名称&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后台运行</span></span><br><span class="line">nohup curl http://<span class="variable">&lt;公网IP&gt;</span>:6800/schedule.json -d project=<span class="variable">&lt;项目名称&gt;</span> -d spider=<span class="variable">&lt;爬虫名称&gt;</span> &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止爬虫</span></span><br><span class="line">curl http://<span class="variable">&lt;公网IP&gt;</span>:6800/cancel.json -d project=<span class="variable">&lt;项目名称&gt;</span> -d job=<span class="variable">&lt;JOBID&gt;</span></span><br></pre></td></tr></table></figure><p>若需要向爬虫传递命令参数和设置DOWNLOAD_DELAY：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:<span class="number">6800</span>/schedule.json -d <span class="attr">project=myproject</span> -d <span class="attr">spider=somespider</span> -d <span class="attr">setting=DOWNLOAD_DELAY=2</span> -d <span class="attr">arg1=val1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的例子，额外传递参数pages=20</span></span><br><span class="line">curl http://<span class="number">49.235</span>.<span class="number">203.45</span>:<span class="number">6800</span>/schedule.json -d <span class="attr">project=LVideoSpider</span> -d <span class="attr">spider=lvideo</span> -d <span class="attr">pages=20</span></span><br></pre></td></tr></table></figure><p><img src="http://image.joelyings.com/2020-01-25_8.png" alt></p><p><img src="http://image.joelyings.com/2020-01-25_9.png" alt></p><p>win系统curl需要<a href="https://curl.haxx.se/download.html#Win64" target="_blank" rel="noopener">下载</a>, 然后将解压目录下的bin目录加入path环境变量中即可在windows的命令行使用</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/u014775723/article/details/86669151" target="_blank" rel="noopener">部署scrapy项目到腾讯云服务器，并操作爬虫</a><br><a href="https://www.cnblogs.com/xiujin/p/11477419.html" target="_blank" rel="noopener">centos7中安装python3</a><br><a href="https://blog.csdn.net/whatday/article/details/98053179" target="_blank" rel="noopener">python3 编译优化 –enable-shared –enable-optimizations</a><br><a href="https://www.cnblogs.com/yesicando/p/11840803.html" target="_blank" rel="noopener">腾讯云Centos7 安装Mysql5.7</a><br><a href="https://scrapyd.readthedocs.io/en/latest/overview.html" target="_blank" rel="noopener">Scrapyd documentation</a><br><a href="https://blog.csdn.net/qq_37958578/article/details/79973265" target="_blank" rel="noopener">Windows安装curl及基本命令</a><br><a href></a></p><!-- rebuild by neat -->]]></content:encoded>
      
      <comments>https://joelying.github.io//blog/%E9%83%A8%E7%BD%B2Scrapy%E9%A1%B9%E7%9B%AE%E5%88%B0%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.html#disqus_thread</comments>
    </item>
    
    <item>
      <title>本地项目提交至GitHub仓库简述</title>
      <link>https://joelying.github.io//blog/%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E6%8F%90%E4%BA%A4%E8%87%B3GitHub%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0.html</link>
      <guid>https://joelying.github.io//blog/%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E6%8F%90%E4%BA%A4%E8%87%B3GitHub%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0.html</guid>
      <pubDate>Thu, 23 Jan 2020 10:02:49 GMT</pubDate>
      <description>
      
        美好的东西从来不会寻求关注
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- build time:Sat Jan 25 2020 14:31:20 GMT+0800 (GMT+08:00) --><img class="joel-img" src="http://image.joelyings.com/2020-01-23_3.jpg"><a id="more"></a><p>适用情况：本地有一个项目，github上新建了一个仓库</p><p>只需要进行下面几步就能把本地项目上传到Github：</p><p>1、在本地创建一个版本库（即文件夹），通过git init把它变成Git仓库</p><p>2、把项目下的文件复制到这个文件夹里面，再通过git add .把项目添加到仓库</p><p>3、再通过git commit -m “注释内容”把项目提交到仓库</p><p>4、看C盘的用户目录下有没有.ssh目录，有的话看下里面有没有id_rsa和id_rsa.pub这两个文件，没有就通过下面命令创建</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ssh-keygen</span> <span class="selector-tag">-t</span> <span class="selector-tag">rsa</span> <span class="selector-tag">-C</span> "<span class="selector-tag">youremail</span>@<span class="keyword">example</span>.<span class="keyword">com</span>"</span><br></pre></td></tr></table></figure><p>然后登录Github，找到右上角的图标，打开点进里面的Settings，再选中里面的SSH and GPG KEYS，点击右上角的New SSH key，然后Title里面随便填，再把刚才<code>id_rsa.pub</code>里面的内容复制到Title下面的Key内容框里面，最后点击Add SSH key，这样就完成了SSH Key的加密</p><p>5、新建一个远程仓库，通过<code>git remote add origin https://github.com/xxx/xxx.git</code>将本地仓库和远程仓库进行关联</p><p>6、最后通过</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">push</span> -u <span class="built_in">origin</span> master</span><br></pre></td></tr></table></figure><p>把本地仓库的项目推送到远程仓库（也就是Github）上，（若新建远程仓库的时候自动创建了README文件会报错，<code>error: failed to push some refs to &#39;https://github.com/xxx/xxx.git&#39;</code> ）</p><p>这时候可以，强制合并</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin <span class="keyword">master</span> <span class="title">--force</span></span><br></pre></td></tr></table></figure><p>以后只需要<code>git push origin master</code>即可</p><!-- rebuild by neat -->]]></content:encoded>
      
      <comments>https://joelying.github.io//blog/%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E6%8F%90%E4%BA%A4%E8%87%B3GitHub%E4%BB%93%E5%BA%93%E7%AE%80%E8%BF%B0.html#disqus_thread</comments>
    </item>
    
    <item>
      <title>pymysql流式游标</title>
      <link>https://joelying.github.io//blog/pymysql%E6%B5%81%E5%BC%8F%E6%B8%B8%E6%A0%87.html</link>
      <guid>https://joelying.github.io//blog/pymysql%E6%B5%81%E5%BC%8F%E6%B8%B8%E6%A0%87.html</guid>
      <pubDate>Thu, 23 Jan 2020 02:25:53 GMT</pubDate>
      <description>
      
        世间万般难事皆可在女子大腿上办妥
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- build time:Sat Jan 25 2020 14:31:20 GMT+0800 (GMT+08:00) --><img class="joel-img" src="http://image.joelyings.com/2020-01-23_1.jpg"><a id="more"></a><p>Python通过pymysql操作向mysql读取千万、百万级别的数据库时</p><p>如果用传统的<code>fetchall()</code>或<code>fetchone()</code>方法，都是先默认在内存里缓存下所有行然后再处理，大量的数据会导致内存资源消耗光，内存容易溢出</p><p>此时则建议使用<code>SSCursor</code>(流式游标)，避免客户端占用大量内存</p><p>这个 cursor 实际上没有缓存下来任何数据，它不会读取所有所有到内存中，它的做法是从储存块中读取记录，并且一条一条返回给你,使用迭代器而不用 fetchall ,即省内存又能很快拿到数据</p><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_pan_url</span><span class="params">(self)</span>:</span></span><br><span class="line">    db = pymysql.connect(host=<span class="string">'localhost'</span>, port=<span class="number">3306</span>, user=<span class="string">'root'</span>, passwd=<span class="string">''</span>, db=<span class="string">''</span>)</span><br><span class="line">    cursor = db.cursor(cursor=pymysql.cursors.SSDictCursor)</span><br><span class="line"></span><br><span class="line">    select_sql = <span class="string">"select `name`, `detail_url`, `pan_title`, `pan_psw`, `pan_real_url` from agepan_wj"</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor.execute(select_sql)</span><br><span class="line">        <span class="comment"># 在处理大量数据时可以分割进行</span></span><br><span class="line">        datas = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> pan <span class="keyword">in</span> datas:</span><br><span class="line">            self.row_lists.append(pan)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'取数据失败'</span>, e)</span><br><span class="line">        db.rollback()</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        cursor.close()</span><br><span class="line">        db.close()</span><br></pre></td></tr></table></figure><p>需要注意的是：</p><p>因为<code>SSCursor</code>是没有缓存的游标，结果集只要没取完，这个<code>connect</code>是不能再处理别的<code>sql</code>，包括另外生成一个<code>cursor</code> 也不行的，如果需要干别的，请另外再生成一个连接对象</p><p>每次读取后处理数据要快，不能超过 60 s，否则<code>mysql</code>将会断开这次连接</p><p>也可以修改<code>SET NET_WRITE_TIMEOUT = xx</code>来增加超时间隔</p><!-- rebuild by neat -->]]></content:encoded>
      
      <comments>https://joelying.github.io//blog/pymysql%E6%B5%81%E5%BC%8F%E6%B8%B8%E6%A0%87.html#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy命令行动态传参给spider</title>
      <link>https://joelying.github.io//blog/Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8A%A8%E6%80%81%E4%BC%A0%E5%8F%82%E7%BB%99spider.html</link>
      <guid>https://joelying.github.io//blog/Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8A%A8%E6%80%81%E4%BC%A0%E5%8F%82%E7%BB%99spider.html</guid>
      <pubDate>Thu, 23 Jan 2020 02:23:33 GMT</pubDate>
      <description>
      
        世间痴情男儿，不论地位高低，大抵都是喜欢女子便是错了，而且希望能一辈子知错不改
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- build time:Sat Jan 25 2020 14:31:20 GMT+0800 (GMT+08:00) --><img class="joel-img" src="http://image.joelyings.com/2020-01-23_2.jpg"><a id="more"></a><h3 id="scrapy命令行执行传递多个参数给spider-动态传参"><a href="#scrapy命令行执行传递多个参数给spider-动态传参" class="headerlink" title="scrapy命令行执行传递多个参数给spider 动态传参"></a>scrapy命令行执行传递多个参数给spider 动态传参</h3><p>在命令行运行scrapy爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spider_name</span><br></pre></td></tr></table></figure><p>若爬虫中有参数可以控制爬取的页数，那么想要在输入命令行命令时传递页数给爬虫，就可以这样做</p><p>在spider中定义一个构造函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pages=None, *args, **kwargs)</span>:</span></span><br><span class="line">    super(LvideoSpider, self).__init__(*args, **kwargs)</span><br><span class="line">    <span class="comment"># super().__init__(**kwargs)</span></span><br><span class="line">    self.pages = pages</span><br><span class="line">    </span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    pages = re.findall(<span class="string">r'当前:1/(\d+)页'</span>, response.text, re.S)[<span class="number">0</span>]</span><br><span class="line">    link = response.meta[<span class="string">'format_page'</span>]</span><br><span class="line">    <span class="comment"># 如果self.pages存在，那么就会代替正则取到的页数</span></span><br><span class="line">    <span class="keyword">if</span> self.pages:</span><br><span class="line">        pages = self.pages</span><br><span class="line">        print(<span class="string">'共'</span> + str(pages) + <span class="string">'页'</span>)</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, int(pages) + <span class="number">1</span>):</span><br><span class="line">        page_link = link.format(page)</span><br><span class="line">        <span class="keyword">yield</span> Request(page_link, callback=self.parse_video_link,</span><br><span class="line">                      meta=&#123;<span class="string">'soucre_name'</span>: response.meta[<span class="string">'soucre_name'</span>],</span><br><span class="line">                            <span class="string">'domin'</span>: response.meta[<span class="string">'domin'</span>],</span><br><span class="line">                            <span class="string">'page_link'</span>: page_link&#125;, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>然后在启动scrapy的时候就可以赋予参数的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spider_name -a pages=<span class="number">10</span></span><br></pre></td></tr></table></figure><p>这样就控制了爬取页数为10页</p><!-- rebuild by neat -->]]></content:encoded>
      
      <comments>https://joelying.github.io//blog/Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8A%A8%E6%80%81%E4%BC%A0%E5%8F%82%E7%BB%99spider.html#disqus_thread</comments>
    </item>
    
    <item>
      <title>mysql保存数据时含有单引号报错方法</title>
      <link>https://joelying.github.io//blog/Python%E6%93%8D%E4%BD%9Cmysql%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E6%97%B6%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%90%AB%E6%9C%89%E5%8D%95%E5%BC%95%E5%8F%B7%E6%8A%A5%E9%94%99%E6%96%B9%E6%B3%95.html</link>
      <guid>https://joelying.github.io//blog/Python%E6%93%8D%E4%BD%9Cmysql%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E6%97%B6%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%90%AB%E6%9C%89%E5%8D%95%E5%BC%95%E5%8F%B7%E6%8A%A5%E9%94%99%E6%96%B9%E6%B3%95.html</guid>
      <pubDate>Wed, 22 Jan 2020 09:22:39 GMT</pubDate>
      <description>
      
        最苦是相思，最远是阴阳
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- build time:Sat Jan 25 2020 14:31:20 GMT+0800 (GMT+08:00) --><img class="joel-img" src="http://image.joelyings.com/2020-01-22_2.jpg"><a id="more"></a><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>mysql在保存视频名称时遇到的数据插入错误，查一下原因发现报错视频名称主要都带有<code>&#39;</code>单引号，会导致SQL语句被截断，从而产生错误</p><p>解决办法如下：</p><p>替换成两个单引号即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = str(name).replace(<span class="string">'\''</span>, <span class="string">'\'\''</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content:encoded>
      
      <comments>https://joelying.github.io//blog/Python%E6%93%8D%E4%BD%9Cmysql%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E6%97%B6%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%90%AB%E6%9C%89%E5%8D%95%E5%BC%95%E5%8F%B7%E6%8A%A5%E9%94%99%E6%96%B9%E6%B3%95.html#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
